# üöÄ D√©ploiement sur Coolify avec Nixpacks

Guide pour d√©ployer JSON Translator sur Coolify en utilisant Nixpacks.

## üìã Pr√©requis

- Serveur Coolify configur√© et accessible
- Ollama install√© quelque part (sur votre PC ou sur le serveur)
- Git repository accessible (GitHub, GitLab, etc.)

---

## üîß Configuration Coolify

### 1. Cr√©er une nouvelle application

Dans Coolify :
1. Cliquez sur **"New Resource"** ‚Üí **"Application"**
2. S√©lectionnez votre repository Git
3. S√©lectionnez la branche (ex: `main` ou `claude/check-github-readiness-...`)
4. **Build Pack** : S√©lectionnez **"Nixpacks"** (d√©tection automatique)

### 2. Variables d'environnement

Dans l'onglet **Environment Variables**, ajoutez :

#### **Option A : Ollama sur votre PC local**

```env
# Requis
OLLAMA_HOST=http://192.168.1.100:11434
PORT=7860
ENVIRONMENT=production

# Optionnel
HOST=0.0.0.0
```

‚ö†Ô∏è **Remplacez `192.168.1.100`** par l'IP locale de votre PC o√π Ollama tourne.

**Trouver votre IP locale** :
```bash
# Sur votre PC avec Ollama
ip addr show | grep "inet " | grep -v 127.0.0.1
# ou
hostname -I
```

**V√©rifier qu'Ollama est accessible** depuis le serveur Coolify :
```bash
# Depuis le serveur Coolify
curl http://192.168.1.100:11434/api/tags
```

#### **Option B : Ollama sur le serveur Coolify**

Si vous installez Ollama directement sur le serveur Coolify (NixOS) :

```env
# Requis
OLLAMA_HOST=http://localhost:11434
PORT=7860
ENVIRONMENT=production
```

**Installation Ollama sur NixOS** :
```nix
# /etc/nixos/configuration.nix
{
  services.ollama = {
    enable = true;
    host = "0.0.0.0";
    port = 11434;
  };
}
```

Puis :
```bash
sudo nixos-rebuild switch
ollama pull mistral  # ou llama2
```

### 3. Configuration r√©seau (Coolify)

1. **Port** : `7860` (ou autre selon votre PORT)
2. **Protocol** : `HTTP`
3. **Domaine** : Configurez un domaine ou utilisez l'IP:port

---

## üöÄ D√©ploiement

### Premier d√©ploiement

1. Dans Coolify, cliquez sur **"Deploy"**
2. Nixpacks va automatiquement :
   - D√©tecter Python 3.11
   - Installer `requirements.txt`
   - Lancer `python json_translator.py`
3. Attendez que le build se termine (~2-5 minutes)
4. L'application sera accessible sur `http://votre-serveur:7860`

### D√©ploiements suivants

√Ä chaque push sur la branche configur√©e, Coolify red√©ploie automatiquement (si activ√©).

---

## ‚úÖ V√©rification

### 1. V√©rifier les logs

Dans Coolify ‚Üí Votre application ‚Üí **Logs** :

```
INFO - Ollama client configured to connect to: http://192.168.1.100:11434
INFO - Launching Gradio interface on 0.0.0.0:7860
INFO - Starting JSON Selective Translator with Ollama
Running on local URL:  http://0.0.0.0:7860
```

### 2. Tester la connexion Ollama

```bash
# Depuis votre navigateur
http://votre-serveur:7860

# Dans l'interface, v√©rifiez que les mod√®les Ollama apparaissent
```

Si vous voyez la liste des mod√®les ‚Üí ‚úÖ Connexion Ollama OK

### 3. Tester une traduction

1. Uploadez un fichier JSON de test
2. S√©lectionnez des cl√©s
3. Choisissez une langue
4. Cliquez "Translate"

---

## üîß D√©pannage

### Probl√®me : "Cannot connect to Ollama"

**Cause** : Coolify ne peut pas atteindre Ollama

**Solutions** :

1. **V√©rifier la connexion r√©seau** :
   ```bash
   # Depuis le serveur Coolify
   curl http://192.168.1.100:11434/api/tags
   ```

2. **V√©rifier le firewall** sur votre PC :
   ```bash
   # Permettre le port 11434
   sudo ufw allow 11434
   # ou
   sudo firewall-cmd --add-port=11434/tcp --permanent
   ```

3. **V√©rifier qu'Ollama √©coute sur toutes les interfaces** :
   ```bash
   # Sur votre PC
   ss -tlnp | grep 11434
   # Doit afficher 0.0.0.0:11434 (pas 127.0.0.1:11434)
   ```

   Si Ollama √©coute sur 127.0.0.1 uniquement :
   ```bash
   # Arr√™ter Ollama
   killall ollama

   # Red√©marrer avec OLLAMA_HOST
   OLLAMA_HOST=0.0.0.0:11434 ollama serve
   ```

### Probl√®me : "No models found"

**Cause** : Aucun mod√®le Ollama install√©

**Solution** :
```bash
# Sur la machine avec Ollama
ollama pull mistral
ollama pull llama2
ollama list  # V√©rifier
```

### Probl√®me : Port d√©j√† utilis√©

**Cause** : Le port 7860 est d√©j√† pris

**Solution** : Changer la variable `PORT` dans Coolify :
```env
PORT=8080
```

### Probl√®me : Application inaccessible

**Cause** : L'app √©coute sur 127.0.0.1 au lieu de 0.0.0.0

**Solution** : V√©rifier que `HOST=0.0.0.0` est configur√© dans les variables d'environnement.

---

## üìä Architecture r√©seau

### Option A : Ollama distant
```
Internet
   ‚Üì
Coolify Serveur (192.168.1.50)
   ‚îî‚îÄ‚îÄ JSON Translator (Port 7860)
       ‚Üì HTTP
Votre PC (192.168.1.100)
   ‚îî‚îÄ‚îÄ Ollama (Port 11434)
```

### Option B : Tout sur Coolify
```
Internet
   ‚Üì
Coolify Serveur (192.168.1.50)
   ‚îú‚îÄ‚îÄ JSON Translator (Port 7860)
   ‚îî‚îÄ‚îÄ Ollama (Port 11434)
       ‚îî‚îÄ‚îÄ Communication localhost
```

---

## üîí S√©curit√©

### Si vous utilisez Ollama distant (Option A)

‚ö†Ô∏è **Ollama n'a pas d'authentification** par d√©faut

**Recommandations** :

1. **Firewall** : Limitez l'acc√®s au port 11434 :
   ```bash
   # Autoriser uniquement le serveur Coolify
   sudo ufw allow from 192.168.1.50 to any port 11434
   ```

2. **VPN/Tailscale** : Utilisez un r√©seau priv√© virtuel

3. **Reverse Proxy** : Ajoutez une authentification avec Nginx/Caddy

---

## üí° Optimisations

### 1. Activer le d√©ploiement automatique

Dans Coolify ‚Üí Votre application ‚Üí **Settings** :
- Cochez "Automatic Deployment"
- Coolify red√©ploiera √† chaque push

### 2. Configurer un domaine

Dans Coolify ‚Üí Votre application ‚Üí **Domains** :
```
json-translator.votre-domaine.com
```

Coolify g√©n√®re automatiquement les certificats SSL (Let's Encrypt).

### 3. Logs persistants

Les logs Gradio sont affich√©s dans Coolify. Pour les logs structur√©s JSON, vous pouvez les exporter :

```bash
# Dans Coolify, acc√©dez au shell du container
# Puis :
cat /app/logs/*.log | grep "translation_completed" | jq .
```

---

## üìö Ressources

- [Documentation Nixpacks](https://nixpacks.com/docs)
- [Documentation Coolify](https://coolify.io/docs)
- [Documentation Ollama](https://ollama.ai)
- [Documentation Gradio](https://gradio.app)

---

## üÜò Support

Si vous rencontrez des probl√®mes :

1. V√©rifiez les logs dans Coolify
2. Testez la connexion Ollama manuellement
3. V√©rifiez les variables d'environnement
4. Consultez ce guide de d√©pannage

---

**Bon d√©ploiement !** üöÄ
